// The Sacalon Lexer
//
// Copyright (c) 2019-2022 Sacalon Foundation
// all rights reserved.

use strings
use keywords
use token
use conv

var _lexme = "" // current lexeme
var current = -1 // current character
var _source = "" // source code
var line = 1 // current line

var _tokens : [Token]?

// check if current character is at the end of the source code
function isAtEnd() : bool {
    if current >= len(_source) {
        return true
    }
    return false
}

// get next character
function advance() : char {
    current = current + 1
    return _source[current]
}

// get next character without advancing
function peek() : char {
    if isAtEnd() {
        return '\0'
    }
    return _source[current + 1]
}
function peek(ahead : int) : char {
    if isAtEnd() {
        return '\0'
    }
    return _source[current + ahead]
}

// match current character with the given character
// and advance if they match
function match(c : char) : bool {
    if isAtEnd() {
        return false
    }
    if _source[current] == c {
        current = current + 1
        return true
    }
    return false
}

// lexer error
function lexer_error(filename:string, msg:string,line:int) {
    var fmt_msg = format("{}: {} ::{}", filename, msg, to_string(line))
}

// scan next token
function scanToken(filename:string){
    var c = advance()
    var tok : Token?

    if c == '(' {
        tok = Token("LEFT_PAREN","(",line)
        append(_tokens,tok)
    } else if c == ')' {
        tok = Token("RIGHT_PAREN",")",line)
        append(_tokens,tok)
    } 
    else if c == '{' {
        tok = Token("LEFT_BRACE", "{", line)
        append(_tokens,tok)
    }
    else if c == '}' {
        tok = Token("RIGHT_BRACE", "}", line)
        append(_tokens,tok)
    }
    else if c == '[' {
        tok = Token("LEFT_BRACKET", "[", line)
        append(_tokens,tok)
    }
    else if c == ']' {
        tok = Token("RIGHT_BRACKET", "]", line)
        append(_tokens,tok)
    }


    else if c == ',' {
        tok = Token("COMMA", ",", line)
        append(_tokens,tok)
    }
    else if c == '.' {
        tok = Token("DOT", ".", line)
        append(_tokens,tok)
    } 
    else if c == ':' {
        tok = Token("COLON", ":", line)
        append(_tokens,tok)
    }


    else if c == '+' {
        tok = Token("PLUS", "+", line)
        append(_tokens,tok)
    } 
    else if c == '-' {
        tok = Token("MINUS", "-", line)
        append(_tokens,tok)
    } 
    else if c == '*' {
        tok = Token("TIMES", "*", line)
        append(_tokens,tok)
    } 
    else if c == '/' {
        if match('/') {
            while peek() != '\n' and (not isAtEnd()) {
                advance()
            }
        } else {
            tok = Token("SLASH", "/", line)
            append(_tokens,tok)
        }
    }
    else if c == '^' {
        tok = Token("POWER", "^", line)
        append(_tokens,tok)
    }
    else if c == '&' {
        tok = Token("AMPERSAND", "&", line)
        append(_tokens,tok)
    }
    else if c == '@' {
        tok = Token("AT", "@", line)
        append(_tokens,tok)
    }


    else if c == '!' {
        if match('=') {
            tok = Token("BANG_EQUAL", "!=", line)
            append(_tokens,tok)
        } else {
            tok = Token("BANG", "!", line)
            append(_tokens,tok)
        }
    } 
    else if c == '=' {
        if match('=') {
            tok = Token("EQUAL_EQUAL", "==", line)
            append(_tokens,tok)
        } else {
            tok = Token("EQUAL", "=", line)
            append(_tokens,tok)
        }
    } 
    else if c == '<' {
        if match('=') {
            tok = Token("LESS_EQUAL", "<=", line)
            append(_tokens,tok)
        } else {
            tok = Token("LESS", "<", line)
            append(_tokens,tok)
        }
    } 
    else if c == '>' {
        if match('=') {
            tok = Token("GREATER_EQUAL", ">=", line)
            append(_tokens,tok)
        } else {
            tok = Token("GREATER", ">", line)
            append(_tokens,tok)
        }
    } 

    // single line string
    else if c == '"' {
        var value = ""

        while peek() != '"' and (not isAtEnd()) {
            value = value + advance()
        }
        if isAtEnd() {
            lexer_error(filename, "Unterminated string", line)
        }
        // the closing "
        advance()

        tok = Token("STRING", value, line)
        append(_tokens,tok)
    }
    // multi-line string
    else if c == '"' and peek() == '"' and peek(2) == '"' {
        var value = ""

        while peek() != '"' and peek(2) != '"' and peek(3) != '"' and (not isAtEnd()) {
            value = value + advance()
        }
        if isAtEnd() {
            lexer_error(filename, "Unterminated string", line)
        }
        
        // skip closing """
        if peek() == '"' and peek(2) == '"' and peek(3) == '"' {
            advance()
        }else {
            lexer_error(filename, "Unterminated string", line)
        }

        tok = Token("STRING", value, line)
        append(_tokens,tok)
    }
    else if c == '\'' {
        var value = ""
        while peek() != '\'' and (not isAtEnd()) {
            value = value + advance()
        }
        if isAtEnd() {
            lexer_error(filename, "Unterminated string", line)
        }
        // the closing '
        advance()

        tok = Token("CHAR", value, line)
        append(_tokens,tok)
    }
    
    else if is_alpha(c) {
        var value = to_string(c)
        while is_alpha(peek()) {
            value = value + advance()
        }
        if check_keyword(value) {
            tok = Token(value, value, line)
            append(_tokens,tok)
        } else {
            tok = Token("NAME", value, line)
            append(_tokens,tok)
        }
    } 
    else if is_number(c) {
        var value = to_string(c)
        while is_number(peek()) {
            value = value + advance()
        }
        tok = Token("INT", value, line)
        append(_tokens,tok)
    }


    else if c == ' ' or c == '\r' or c == '\t' {
        scanToken(filename)
    } 
    else if c == '\n' {
        tok = Token("NEWLINE", "", line)
        append(_tokens,tok)
        line = line + 1
    }
}

function tokenizer(filename:string,src:string): [Token] {
    _source = src
    while isAtEnd() == false {
        scanToken(filename)
    }
    return _tokens
}
